{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = np.zeros(15)\n",
    "\n",
    "# A stores the actions\n",
    "A = ['up', 'down', 'right', 'left']\n",
    "\n",
    "# T stores the transition: state i take action j --> T[i][j]\n",
    "T = []\n",
    "T.append([0, 0, 0, 0])\n",
    "T.append([1, 5, 2, 0])\n",
    "T.append([2, 6, 3, 1])\n",
    "T.append([3, 7, 3, 2])\n",
    "T.append([0, 8, 5, 4])\n",
    "T.append([1, 9, 6, 4])\n",
    "T.append([2, 10, 7, 5])\n",
    "T.append([3, 11, 7, 6])\n",
    "T.append([4, 12, 9, 8])\n",
    "T.append([5, 13, 10, 8])\n",
    "T.append([6, 14, 11, 9])\n",
    "T.append([7, 0, 11, 10])\n",
    "T.append([8, 12, 13, 12])\n",
    "T.append([9, 13, 14, 12])\n",
    "T.append([10, 14, 0, 13])\n",
    "\n",
    "# R stores the reward\n",
    "R = []\n",
    "R.append([0, 0, 0, 0])\n",
    "for i in range(14):\n",
    "    R.append([-1, -1, -1, -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random policy\n",
    "def random_policy(S):\n",
    "    return np.random.randint(4)\n",
    "\n",
    "total_reward = 0\n",
    "agent = np.random.randint(15)\n",
    "print(agent)\n",
    "while agent:\n",
    "    action = random_policy(agent)\n",
    "    print(A[action])\n",
    "    total_reward += R[agent][action]\n",
    "    agent = T[agent][action]\n",
    "    print(agent)\n",
    "    \n",
    "print(total_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# policy iteration\n",
    "\n",
    "# V stores the value\n",
    "V = np.zeros(15)\n",
    "\n",
    "# P stores the policy\n",
    "P = []\n",
    "for i in range(15):\n",
    "    P.append([0.25, 0.25, 0.25, 0.25])\n",
    "\n",
    "theda = 0.01\n",
    "delta = 1\n",
    "gamma = 0.5\n",
    "\n",
    "\n",
    "def policy_evaluation():\n",
    "    global delta, theda, gamma, V, P\n",
    "    while (delta > theda):\n",
    "        delta = 0\n",
    "        temp = np.zeros(15)\n",
    "        for s in range(15):  \n",
    "            for a in range(4):\n",
    "                temp[s] +=  P[s][a] * (R[s][a] + gamma * V[T[s][a]])\n",
    "            delta = max(delta, abs(V[s] - temp[s]))\n",
    "        V = temp\n",
    "    print(V)\n",
    "\n",
    "def policy_improvement():\n",
    "    global delta, theda, gamma, V, P\n",
    "    policy_stable = True\n",
    "    temp = np.zeros((15, 4))\n",
    "    \n",
    "    for s in range(15):\n",
    "        if not s:\n",
    "            continue\n",
    "        policy_action = argmax(P[s])\n",
    "        v = np.array([V[T[s][0]], V[T[s][1]], V[T[s]][2], V[T[s][3]]])\n",
    "        best_action = argmax(R[s] + v * gamma)\n",
    "        temp[s][best_action] = 1\n",
    "        if (best_action != policy_action):\n",
    "            policy_stable = False \n",
    "    \n",
    "    P = temp\n",
    "    \n",
    "    return policy_stable\n",
    "\n",
    "def policy_iteration():\n",
    "    while True:\n",
    "        policy_evaluation()\n",
    "        is_stable = policy_improvement()\n",
    "        if is_stable:\n",
    "            break\n",
    "    return\n",
    "\n",
    "policy_iteration()\n",
    "print(P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just 2 robot\n",
    "\n",
    "# S = np(15*15)\n",
    "A = ['up up', 'up down', 'up right', 'up left', 'down up', 'down down', 'down right', 'down left',\n",
    "     'right up', 'right down', 'right right', 'right left', 'left up', 'left down', 'left right', 'left left']\n",
    "T = []\n",
    "T.append([0, 0, 0, 0])\n",
    "T.append([1, 5, 2, 0])\n",
    "T.append([2, 6, 3, 1])\n",
    "T.append([3, 7, 3, 2])\n",
    "T.append([0, 8, 5, 4])\n",
    "T.append([1, 9, 6, 4])\n",
    "T.append([2, 10, 7, 5])\n",
    "T.append([3, 11, 7, 6])\n",
    "T.append([4, 12, 9, 8])\n",
    "T.append([5, 13, 10, 8])\n",
    "T.append([6, 14, 11, 9])\n",
    "T.append([7, 0, 11, 10])\n",
    "T.append([8, 12, 13, 12])\n",
    "T.append([9, 13, 14, 12])\n",
    "T.append([10, 14, 0, 13])\n",
    "\n",
    "T_new = np.zeros((15*15, 16))\n",
    "for i in range(15):\n",
    "    for j in range(15):\n",
    "        for a1 in range(4):\n",
    "            for a2 in range(4):\n",
    "                i_new = T[i][a1]\n",
    "                j_new = T[j][a2]\n",
    "                T_new[i*15+j][a1*4+a2] = i_new*15 + j_new\n",
    "\n",
    "R = []\n",
    "for i in range(15):\n",
    "    for j in range(15):\n",
    "        if i == 0 and j == 0:\n",
    "            R.append([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
    "        elif i == 0:\n",
    "            R.append([-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1])\n",
    "        elif j == 0:\n",
    "            R.append([-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1])\n",
    "        else :\n",
    "            R.append([-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1] * 2)\n",
    "            \n",
    "for i in range(15):\n",
    "    for j in range(15):\n",
    "        if i == 0 or j == 0:\n",
    "            continue\n",
    "        for a1 in range(4):\n",
    "            for a2 in range(4):\n",
    "                if T[i][a1] == T[j][a2]:\n",
    "                    R[i*15+j][a1*4+a2] = -100\n",
    "                    \n",
    "# policy iteration\n",
    "\n",
    "# V stores the value\n",
    "V = np.zeros(15 * 15)\n",
    "\n",
    "# P stores the policy\n",
    "P = []\n",
    "for i in range(15 * 15):\n",
    "    P.append([1/16, 1/16, 1/16, 1/16, 1/16, 1/16, 1/16, 1/16, 1/16, 1/16, 1/16, 1/16, 1/16, 1/16, 1/16, 1/16])\n",
    "theda = 0.01\n",
    "delta = 1\n",
    "gamma = 0.5\n",
    "\n",
    "\n",
    "def policy_evaluation():\n",
    "    global delta, theda, gamma, V, P\n",
    "    while (delta > theda):\n",
    "        delta = 0\n",
    "        temp = np.zeros(15*15)\n",
    "        for s in range(15*15):  \n",
    "            for a in range(16):\n",
    "                temp[s] = temp[s] + P[s][a] * (R[s][a] + gamma *1)\n",
    "            delta = max(delta, abs(V[s] - temp[s]))\n",
    "        V = temp\n",
    "    print(V)\n",
    "\n",
    "def policy_improvement():\n",
    "    global delta, theda, gamma, V, P\n",
    "    policy_stable = True\n",
    "    temp = np.zeros((15*15, 16))\n",
    "    \n",
    "    for s in range(15*15):\n",
    "        if not s:\n",
    "            continue\n",
    "        policy_action = argmax(P[s])\n",
    "        v = np.array([V[T[s][0]], V[T[s][1]], V[T[s]][2], V[T[s][3]]])\n",
    "        best_action = argmax(R[s] + v * gamma)\n",
    "        temp[s][best_action] = 1\n",
    "        if (best_action != policy_action):\n",
    "            policy_stable = False\n",
    "    \n",
    "    P = temp\n",
    "    \n",
    "    return policy_stable\n",
    "\n",
    "def policy_iteration():\n",
    "    while True:\n",
    "        policy_evaluation()\n",
    "        is_stable = policy_improvement()\n",
    "        if is_stable:\n",
    "            break\n",
    "    return\n",
    "\n",
    "policy_iteration()\n",
    "print(P)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
